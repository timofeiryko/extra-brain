# Intro

`hello.py`

```python
import spacy

# Create a blank English nlp object

nlp = spacy.blank("en")
```

The nlp object contains the **processing pipeline** and language-specific rulles.

Using this object, we can create another **Doc object**:

```python
# Created by processing a string of text with the nlp object

doc = nlp("Hello world!")
```

Doc is a "list" of **Token objects** (words, numbers or punctuation marks).
You an iterate through the Doc, get tokens by index and even slice it in classical python way! The result of slcing is a **Span object**.

Span and token objects have an attibute `text` .

Token has an atribute `i` (index of the token) and other attrs. Attributes, that return strings usually end with an underscore – attributes without the underscore return an integer ID value.

**True/False attributes of the Token object**:
- `is_alpha`
- `is_punct`
- `like_num` ("one" is also a number)

*There are some pitfalls in  like_num attribute: "the one" etc, "it's 2 late", "this is 4 you"*

These tokens are **lexical**: they don't depend on the context!

# Pipelines

`piplines.py`

Models that enable spaCy to predict linguistic attributes **in context**:
-   Part-of-speech tags
-   Syntactic dependencies
-   Named entities

Trained on labeled example texts сan be updated with more examples to fine-tune predictions on your **specific** data.

To load trained pipelines:

```bash
$ python -m spacy download en_core_web_sm
```

```python
nlp = spacy.load("en_core_web_sm")
```

`en_core_web_sm`  is a small English pipeline that supports all core capabilities and is trained on web text.

The pipeline package contains:
- Binary weights - enable spaCy to make predictions
- Vocabulary
- Meta information
- Configuration file used to train it (tells spaCy which language class to use and how to configure the processing pipeline)

Usefull token attrs, generated by the pipline:
- `pos_` - predicted part-of-speech tag
- `dep_` - predicted dependency label
- `head` - syntactic head token (the parent token this word is attached to)

If token ends with `_` - it returns string, otherwise - int ID (usually).

```python
doc = nlp("She ate the pizza")  

for token in doc:
# Print the text and the predicted part-of-speech tag
	print(token.text, token.pos_, token.dep_, token.head.text)
```

### Dependency label scheme
![Visualization of the dependency graph for 'She ate the pizza'](https://course.spacy.io/dep_example.png)

- nominal objet (she)
- direct object (pizza)

### Predicting Named Entities
Named entities are "real world objects" that are assigned a name.

```python
# Iterate over the predicted entities
for ent in doc.ents:
# Print the entity text and its label
	print(ent.text, ent.label_)
```

`doc.ents` returns an iterator of Span objects, each of the span objects has `text` and `span_` attrs.

### Expain method

If you don't know, what some tag or label means, you can quicly check it with `spacy.explain`! It also works for part-of-speech tags and dependency labels.

```python
spacy.explain("GPE")
```

```out
'Countries, cities, states'
```

# Rule-based matching

`matching.py`

Spacy matcher is better than just regex, because it can:
- Match on Doc bjects, not just strings
- Match on tokens and token attributes
- Use a model's predictions

**Match patterns are lists of dictionaries**, one dictionary per token.

Each dictionary looks like `{'TOKEN 1': 'VALUE 1', 'TOKEN 2': VALUE 2}`

```python
# Import the Matcher
from spacy.matcher import Matcher

# Load a pipeline and create the nlp object
nlp = spacy.load("en_core_web_sm")

# Initialize the matcher with the shared vocabulary
matcher = Matcher(nlp.vocab)

# Add the pattern to the matcher
pattern = [{"TEXT": "iPhone"}, {"TEXT": "X"}]

matcher.add("IPHONE_PATTERN", [pattern])

# Process some text
doc = nlp("Upcoming iPhone X release date leaked")

# Call the matcher on the doc
matches = matcher(doc)
```

Operators can be added with `'OP'` key:

```python
pattern = [
	{"LEMMA": "buy"},
	{"POS": "DET", "OP": "?"}, # optional: match 0 or 1 times
	{"POS": "NOUN"}
]

doc = nlp("I bought a smartphone. Now I'm buying apps.")
```

```out
bought a smartphone
buying apps
```

Possible values of `'OP'`:


|Key|Description|
|---|---|
|`{"OP": "!"}`|Negation: match 0 times|
|`{"OP": "?"}`|Optional: match 0 or 1 times|
|`{"OP": "+"}`|Match 1 or more times|
|`{"OP": "*"}`|Match 0 or more times|

When you call the matcher on a doc, it returns a **list of tuples**. Each tupple consists of 3 values:
-   `match_id`: hash value of the pattern name
-   `start`: start index of matched _span_
-   `end`: end index of matched _span_

```python
# Iterate over the matches
for match_id, start, end in matches:
	# Get the matched _span_
	matched_span = doc[start:end]
	print(matched_span.text)
```

